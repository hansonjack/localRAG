## langchain-chatchat ç›®å‰(2023-11)è¿˜æ²¡å¯¹chatglm-cppé¡¹ç›®æ”¯æŒï¼Œä¸èƒ½å¾ˆæ„‰å¿«åœ°åˆ©ç”¨æ¶ˆè´¹çº§çš„ç”µè„‘è¿›è¡Œå¤§æ¨¡å‹ç©è€ã€‚æ•…åŸºäºlangchain-chatcaht(v0.2.7)åšäº†ä¸€äº›ä¿®æ”¹ï¼Œæ”¯æŒchatglm-cppé¡¹ç›®æ‰€æ”¯æŒçš„LLMã€‚

## ç›®å‰å¯¹langchain-chatchatç›®æ”¹åŠ¨å¤šä¸€äº›ï¼Œåé¢ä¼šæŠŠä¿®æ”¹éƒ¨åˆ†å•ç‹¬å‡ºæ¥ï¼Œä¸å½±å“langchain-chatchatï¼Œè¿™æ ·å¯ä»¥æŒç»­æ›´æ–°langchain-chatchatç‰ˆæœ¬ã€‚
PSï¼šlangchain-chatchat æ‰€æ”¯æŒçš„çº¿ä¸ŠLLM API å‡ ä¹éƒ½æ˜¯è¦é’±çš„ï¼Œä¸”éƒ½è¦æ³¨å†Œå®ååˆ¶ï¼Œæ¶å¿ƒã€‚æ•…æœ‰æ­¤é¡¹ç›®

---

## ç›®å½•

* [ä»‹ç»](README.md#ä»‹ç»)
* [è§£å†³çš„ç—›ç‚¹](README.md#è§£å†³çš„ç—›ç‚¹)
* [å¿«é€Ÿä¸Šæ‰‹](README.md#å¿«é€Ÿä¸Šæ‰‹)
  * [1. å®‰è£…éƒ¨ç½²langchain-chatchat](README.md#1-å®‰è£…éƒ¨ç½²langchain-chatchat)
  * [2. ä¿®æ”¹/å¢åŠ çš„å†…å®¹](README.md#2-ä¿®æ”¹/å¢åŠ çš„å†…å®¹)
  * [3. langchain-chatchatä»£ç ç»“æ„ç®€å•æ¢³ç†](README.md#3-langchain-chatchatä»£ç ç»“æ„ç®€å•æ¢³ç†)



## ä»‹ç»

ğŸ¤–ï¸ ä¸€ç§åˆ©ç”¨ [langchain](https://github.com/hwchase17/langchain) æ€æƒ³å®ç°çš„åŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„é—®ç­”åº”ç”¨ï¼Œç›®æ ‡æœŸæœ›å»ºç«‹ä¸€å¥—å¯¹ä¸­æ–‡åœºæ™¯ä¸å¼€æºæ¨¡å‹æ”¯æŒå‹å¥½ã€å¯ç¦»çº¿è¿è¡Œçš„çŸ¥è¯†åº“é—®ç­”è§£å†³æ–¹æ¡ˆã€‚

ğŸ’¡ å— [GanymedeNil](https://github.com/GanymedeNil) çš„é¡¹ç›® [document.ai](https://github.com/GanymedeNil/document.ai) å’Œ [AlexZhangji](https://github.com/AlexZhangji) åˆ›å»ºçš„ [ChatGLM-6B Pull Request](https://github.com/THUDM/ChatGLM-6B/pull/216) å¯å‘ï¼Œå»ºç«‹äº†å…¨æµç¨‹å¯ä½¿ç”¨å¼€æºæ¨¡å‹å®ç°çš„æœ¬åœ°çŸ¥è¯†åº“é—®ç­”åº”ç”¨ã€‚æœ¬é¡¹ç›®çš„æœ€æ–°ç‰ˆæœ¬ä¸­é€šè¿‡ä½¿ç”¨ [FastChat](https://github.com/lm-sys/FastChat) æ¥å…¥ Vicuna, Alpaca, LLaMA, Koala, RWKV ç­‰æ¨¡å‹ï¼Œä¾æ‰˜äº [langchain](https://github.com/langchain-ai/langchain) æ¡†æ¶æ”¯æŒé€šè¿‡åŸºäº [FastAPI](https://github.com/tiangolo/fastapi) æä¾›çš„ API è°ƒç”¨æœåŠ¡ï¼Œæˆ–ä½¿ç”¨åŸºäº [Streamlit](https://github.com/streamlit/streamlit) çš„ WebUI è¿›è¡Œæ“ä½œã€‚

âœ… ä¾æ‰˜äºæœ¬é¡¹ç›®æ”¯æŒçš„å¼€æº LLM ä¸ Embedding æ¨¡å‹ï¼Œæœ¬é¡¹ç›®å¯å®ç°å…¨éƒ¨ä½¿ç”¨**å¼€æº**æ¨¡å‹**ç¦»çº¿ç§æœ‰éƒ¨ç½²**ã€‚ä¸æ­¤åŒæ—¶ï¼Œæœ¬é¡¹ç›®ä¹Ÿæ”¯æŒ OpenAI GPT API çš„è°ƒç”¨ï¼Œå¹¶å°†åœ¨åç»­æŒç»­æ‰©å……å¯¹å„ç±»æ¨¡å‹åŠæ¨¡å‹ API çš„æ¥å…¥ã€‚

â›“ï¸ æœ¬é¡¹ç›®å®ç°åŸç†å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿‡ç¨‹åŒ…æ‹¬åŠ è½½æ–‡ä»¶ -> è¯»å–æ–‡æœ¬ -> æ–‡æœ¬åˆ†å‰² -> æ–‡æœ¬å‘é‡åŒ– -> é—®å¥å‘é‡åŒ– -> åœ¨æ–‡æœ¬å‘é‡ä¸­åŒ¹é…å‡ºä¸é—®å¥å‘é‡æœ€ç›¸ä¼¼çš„ `top k`ä¸ª -> åŒ¹é…å‡ºçš„æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡å’Œé—®é¢˜ä¸€èµ·æ·»åŠ åˆ° `prompt`ä¸­ -> æäº¤ç»™ `LLM`ç”Ÿæˆå›ç­”ã€‚

ğŸ“º [åŸç†ä»‹ç»è§†é¢‘](https://www.bilibili.com/video/BV13M4y1e7cN/?share_source=copy_web&vd_source=e6c5aafe684f30fbe41925d61ca6d514)

![å®ç°åŸç†å›¾](img/langchain+chatglm.png)

ä»æ–‡æ¡£å¤„ç†è§’åº¦æ¥çœ‹ï¼Œå®ç°æµç¨‹å¦‚ä¸‹ï¼š

![å®ç°åŸç†å›¾2](img/langchain+chatglm2.png)

ğŸš© æœ¬é¡¹ç›®æœªæ¶‰åŠå¾®è°ƒã€è®­ç»ƒè¿‡ç¨‹ï¼Œä½†å¯åˆ©ç”¨å¾®è°ƒæˆ–è®­ç»ƒå¯¹æœ¬é¡¹ç›®æ•ˆæœè¿›è¡Œä¼˜åŒ–ã€‚

ğŸŒ [AutoDL é•œåƒ](https://www.codewithgpu.com/i/chatchat-space/Langchain-Chatchat/Langchain-Chatchat) ä¸­ `v11` ç‰ˆæœ¬æ‰€ä½¿ç”¨ä»£ç å·²æ›´æ–°è‡³æœ¬é¡¹ç›® `v0.2.7` ç‰ˆæœ¬ã€‚

ğŸ³ [Docker é•œåƒ](registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.6) å·²ç»æ›´æ–°åˆ° ```0.2.7``` ç‰ˆæœ¬ã€‚

ğŸŒ² ä¸€è¡Œå‘½ä»¤è¿è¡Œ Docker ï¼š

```shell
docker run -d --gpus all -p 80:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.7
```

ğŸ§© æœ¬é¡¹ç›®æœ‰ä¸€ä¸ªéå¸¸å®Œæ•´çš„[Wiki](https://github.com/chatchat-space/Langchain-Chatchat/wiki/) ï¼Œ READMEåªæ˜¯ä¸€ä¸ªç®€å•çš„ä»‹ç»ï¼Œ__ä»…ä»…æ˜¯å…¥é—¨æ•™ç¨‹ï¼Œèƒ½å¤ŸåŸºç¡€è¿è¡Œ__ã€‚ å¦‚æœä½ æƒ³è¦æ›´æ·±å…¥çš„äº†è§£æœ¬é¡¹ç›®ï¼Œæˆ–è€…æƒ³å¯¹æœ¬é¡¹ç›®åšå‡ºè´¡çŒ®ã€‚è¯·ç§»æ­¥ [Wiki](https://github.com/chatchat-space/Langchain-Chatchat/wiki/)  ç•Œé¢

## è§£å†³çš„ç—›ç‚¹

è¯¥é¡¹ç›®æ˜¯ä¸€ä¸ªå¯ä»¥å®ç° __å®Œå…¨æœ¬åœ°åŒ–__æ¨ç†çš„çŸ¥è¯†åº“å¢å¼ºæ–¹æ¡ˆ, é‡ç‚¹è§£å†³æ•°æ®å®‰å…¨ä¿æŠ¤ï¼Œç§åŸŸåŒ–éƒ¨ç½²çš„ä¼ä¸šç—›ç‚¹ã€‚
æœ¬å¼€æºæ–¹æ¡ˆé‡‡ç”¨```Apache License```ï¼Œå¯ä»¥å…è´¹å•†ç”¨ï¼Œæ— éœ€ä»˜è´¹ã€‚

æˆ‘ä»¬æ”¯æŒå¸‚é¢ä¸Šä¸»æµçš„æœ¬åœ°å¤§é¢„è¨€æ¨¡å‹å’ŒEmbeddingæ¨¡å‹ï¼Œæ”¯æŒå¼€æºçš„æœ¬åœ°å‘é‡æ•°æ®åº“ã€‚
æ”¯æŒåˆ—è¡¨è¯¦è§[Wiki](https://github.com/chatchat-space/Langchain-Chatchat/wiki/)


## å¿«é€Ÿä¸Šæ‰‹

### 1. å®‰è£…éƒ¨ç½²langchain-chatchat

è¯¦ç»†æ•™ç¨‹å‚è€ƒ[langchain-chatchat](https://github.com/chatchat-space/Langchain-Chatchat)

### 2. ä¿®æ”¹/å¢åŠ çš„å†…å®¹

- 1ã€æŠŠchatglm-cpp å°è£…æˆopenai apiçš„è°ƒç”¨æ–¹å¼
- 2ã€åˆ©ç”¨chatchaté¡¹ç›®æœ¬èº«æ”¯æŒçš„openai apièƒ½åŠ›ï¼Œè°ƒç”¨ç¬¬1æ­¥çš„APIæ¥å£ï¼Œè·å–LLMèƒ½åŠ›
- å…·ä½“æ­¥éª¤ï¼š
	- 1ã€æŠŠchatglm-cpp apié›†æˆåˆ°chatchat ä¸­
	  collapsed:: true
		- 1.1ã€åœ¨chatchat é¡¹ç›®ä¸­serverç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªpyæ–‡ä»¶,å‘½å:chatglm_cpp_api.py,æŠŠchatglm-cpp é¡¹ç›®ä¸­çš„openai_api.pyä»£ç å¤åˆ¶åˆ°chatglm_cpp_api.pyä¸­ï¼Œç„¶ååšä¸‹ä¿®æ”¹ã€‚ä¿®æ”¹åå¦‚ä¸‹ï¼š
		  collapsed:: true
			-
			  ```
			  import asyncio
			  import logging
			  import time
			  from typing import List, Literal, Optional, Union
			  
			  import chatglm_cpp
			  from fastapi import FastAPI, HTTPException, Request, status
			  from fastapi.middleware.cors import CORSMiddleware
			  from pydantic import BaseModel, Field
			  from sse_starlette.sse import EventSourceResponse
			  from configs.model_config import VLLM_MODEL_DICT
			  logging.basicConfig(level=logging.INFO, format=r"%(asctime)s - %(module)s - %(levelname)s - %(message)s")
			  
			  
			  
			  
			  
			  class ChatMessage(BaseModel):
			      role: Literal["system", "user", "assistant"]
			      content: str
			  
			  
			  class DeltaMessage(BaseModel):
			      role: Optional[Literal["system", "user", "assistant"]] = None
			      content: Optional[str] = None
			  
			  
			  class ChatCompletionRequest(BaseModel):
			      model: str = "default-model"
			      messages: List[ChatMessage]
			      temperature: float = Field(default=0.95, ge=0.0, le=2.0)
			      top_p: float = Field(default=0.7, ge=0.0, le=1.0)
			      stream: bool = False
			      max_tokens: int = Field(default=2048, ge=0)
			  
			      model_config = {
			          "json_schema_extra": {"examples": [{"model": "default-model", "messages": [{"role": "user", "content": "ä½ å¥½"}]}]}
			      }
			  
			  class ChatCompletionRequest1(BaseModel):
			      model: str = "default-model"
			      messages: List[ChatMessage]
			      temperature: float = Field(default=0.95, ge=0.0, le=2.0)
			      max_tokens: int = Field(default=2048, ge=0)
			      stream: bool = False
			      n: int = Field(default=1, ge=0)
			      top_p: float = Field(default=0.7, ge=0.0, le=1.0)
			  
			  
			  class ChatCompletionResponseChoice(BaseModel):
			      index: int = 0
			      message: ChatMessage
			      finish_reason: Literal["stop", "length"] = "stop"
			  
			  
			  class ChatCompletionResponseStreamChoice(BaseModel):
			      index: int = 0
			      delta: DeltaMessage
			      finish_reason: Optional[Literal["stop", "length"]] = None
			  
			  
			  class ChatCompletionResponse(BaseModel):
			      id: str = "chatcmpl"
			      model: str = "default-model"
			      object: Literal["chat.completion", "chat.completion.chunk"]
			      created: int = Field(default_factory=lambda: int(time.time()))
			      choices: Union[List[ChatCompletionResponseChoice], List[ChatCompletionResponseStreamChoice]]
			  
			      model_config = {
			          "json_schema_extra": {
			              "examples": [
			                  {
			                      "id": "chatcmpl",
			                      "model": "default-model",
			                      "object": "chat.completion",
			                      "created": 1691166146,
			                      "choices": [
			                          {
			                              "index": 0,
			                              "message": {"role": "assistant", "content": "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"},
			                              "finish_reason": "stop",
			                          }
			                      ],
			                  }
			              ]
			          }
			      }
			  
			  app = FastAPI()
			  app.add_middleware(
			      CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
			  )
			  pipeline = chatglm_cpp.Pipeline(VLLM_MODEL_DICT['chatglm-cpp'])
			  lock = asyncio.Lock()
			  
			  
			  def stream_chat(history, body):
			      yield ChatCompletionResponse(
			          object="chat.completion.chunk",
			          choices=[ChatCompletionResponseStreamChoice(delta=DeltaMessage(role="assistant"))],
			      )
			  
			      for piece in pipeline.chat(
			          history,
			          max_length=body.max_tokens,
			          do_sample=body.temperature > 0,
			          top_p=body.top_p,
			          temperature=body.temperature,
			          num_threads=0,
			          stream=True,
			      ):
			          yield ChatCompletionResponse(
			              object="chat.completion.chunk",
			              choices=[ChatCompletionResponseStreamChoice(delta=DeltaMessage(content=piece))],
			          )
			  
			      yield ChatCompletionResponse(
			          object="chat.completion.chunk",
			          choices=[ChatCompletionResponseStreamChoice(delta=DeltaMessage(), finish_reason="stop")],
			      )
			  
			  
			  async def stream_chat_event_publisher(history, body):
			      output = ""
			      try:
			          async with lock:
			              for chunk in stream_chat(history, body):
			                  await asyncio.sleep(0)  # yield control back to event loop for cancellation check
			                  output += chunk.choices[0].delta.content or ""
			                  yield chunk.model_dump_json(exclude_unset=True)
			          logging.info(f'prompt: "{history[-1]}", stream response: "{output}"')
			      except asyncio.CancelledError as e:
			          logging.info(f'prompt: "{history[-1]}", stream response (partial): "{output}"')
			          raise e
			  
			  # @app.middleware("http")
			  # async def log_request(request: Request, call_next):
			  #     # æ‰“å°è¯·æ±‚å‚æ•°
			  #     print(f"è¯·æ±‚å‚æ•°: {await request.json()}")
			      
			  #     response = await call_next(request)
			  #     return response
			  
			  @app.post("/v1/chat/completions")
			  async def create_chat_completion(body: ChatCompletionRequest) -> ChatCompletionResponse:
			      # ignore system messages
			      history = [msg.content for msg in body.messages if msg.role != "system"]
			      if len(history) % 2 != 1:
			          raise HTTPException(status.HTTP_400_BAD_REQUEST, "invalid history size")
			  
			      if body.stream:
			          generator = stream_chat_event_publisher(history, body)
			          return EventSourceResponse(generator)
			  
			      output = pipeline.chat(
			          history=history,
			          max_length=body.max_tokens,
			          do_sample=body.temperature > 0,
			          top_p=body.top_p,
			          temperature=body.temperature,
			      )
			      logging.info(f'prompt: "{history[-1]}", sync response: "{output}"')
			  
			      return ChatCompletionResponse(
			          object="chat.completion",
			          choices=[ChatCompletionResponseChoice(message=ChatMessage(role="assistant", content=output))],
			      )
			  
			  
			  class ModelCard(BaseModel):
			      id: str
			      object: Literal["model"] = "model"
			      owned_by: str = "owner"
			      permission: List = []
			  
			  
			  class ModelList(BaseModel):
			      object: Literal["list"] = "list"
			      data: List[ModelCard] = []
			  
			      model_config = {
			          "json_schema_extra": {
			              "examples": [
			                  {
			                      "object": "list",
			                      "data": [{"id": "gpt-3.5-turbo", "object": "model", "owned_by": "owner", "permission": []}],
			                  }
			              ]
			          }
			      }
			  
			  
			  @app.get("/v1/models")
			  async def list_models() -> ModelList:
			      return ModelList(data=[ModelCard(id="gpt-3.5-turbo")])
			  
			  ```
		- 1.2ã€ä¿®æ”¹startup.pyï¼ŒåŠ å…¥chatglm_cpp api çš„å¯åŠ¨
			- å¢åŠ ä¸€ä¸ªserverå¯åŠ¨å‡½æ•°
			-
			  ```
			  def run_chatglmCpp_api_server(started_event: mp.Event = None, run_mode: str = None):
			      import uvicorn
			      from server.chatglm_cpp_api import app
			  
			      _set_app_event(app, started_event)
			  
			      host = CHATGLM_CPP_SERVER["host"]
			      port = CHATGLM_CPP_SERVER["port"]
			  
			      uvicorn.run(app, host=host, port=port)
			  ```
			- æ·»åŠ ä¸€ä¸ªrun_chatglmCpp_api_serverçš„å¯åŠ¨å‚æ•°:chatglmCPPapi
				-
				  ```
				  if args.chatglmCPPapi:
				          process = Process(
				              target=run_chatglmCpp_api_server,
				              name=f"chatglmCPP_API Server",
				              kwargs=dict(started_event=chatglmCPPapi_started, run_mode=run_mode),
				              daemon=True,
				          )
				          processes["chatglmCPPapi"] = process
				  ```
				- åœ¨-aå‚æ•°ï¼Œ-llm_apiå‚æ•°å¤„æŠŠè¯¥å‚æ•°chatglmCPPapièµ‹å€¼ä¸ºtrue
				-
				  ```
				  chatglmCPPapi_started = manager.Event()
				      if args.chatglmCPPapi:
				          process = Process(
				              target=run_chatglmCpp_api_server,
				              name=f"chatglmCPP_API Server",
				              kwargs=dict(started_event=chatglmCPPapi_started, run_mode=run_mode),
				              daemon=True,
				          )
				          processes["chatglmCPPapi"] = process
				  ```
				-
				  ```
				  if args.all_webui:
				          args.openai_api = True
				          args.model_worker = True
				          args.api = True
				          args.api_worker = True
				          args.webui = True
				          args.chatglmCPPapi = True #åœ¨-aå‚æ•°ä¸­æŠŠè¯¥å‚æ•°è®¾ç½®ä¸ºçœŸ
				  ```
				- å¢åŠ å¯¹chatglm-cpp apiçš„äº‹åŠ¡æ§åˆ¶
					-
					  ```
					  if p:= processes.get("chatglmCPPapi"):
					      p.start()
					      p.name = f"{p.name} ({p.pid})"
					      chatglmCPPapi_started.wait() # ç­‰å¾…chatglmCPPapiå¯åŠ¨å®Œæˆ
					  ```
			-
	- 2ã€ä¿®æ”¹server/chat/openai_chat.py,ä½¿å¾—å…¶openai_chatçš„ä¼ äººå‚æ•°ç¬¦åˆchatglm cpp å°è£…å¥½çš„openaiapiçš„ä¼ å…¥å‚æ•°
	  collapsed:: true
		-
		  ```
		  from fastapi.responses import StreamingResponse
		  from typing import List, Optional,Literal
		  import openai
		  from configs import LLM_MODELS, logger, log_verbose
		  from server.utils import get_model_worker_config, fschat_openai_api_address
		  from pydantic import BaseModel,Field
		  from server.db.repository import add_chat_history_to_db, update_chat_history
		  import json
		  
		  
		  class OpenAiMessage(BaseModel):
		      role: str = "user"
		      content: str = "hello"
		  
		  
		  class OpenAiChatMsgIn(BaseModel):
		      model: str = LLM_MODELS[0]
		      messages: List[OpenAiMessage]
		      temperature: float = 0.7
		      n: int = 1
		      max_tokens: Optional[int] = None
		      stop: List[str] = []
		      stream: bool = False
		      presence_penalty: int = 0
		      frequency_penalty: int = 0
		  
		  ###å¢åŠ å†…å®¹
		  class ChatMessage(BaseModel):
		      role: Literal["system", "user", "assistant"]
		      content: str
		  class ChatCompletionRequest(BaseModel):
		      model: str = "default-model"
		      messages: List[ChatMessage]
		      temperature: float = Field(default=0.95, ge=0.0, le=2.0)
		      top_p: float = Field(default=0.7, ge=0.0, le=1.0)
		      stream: bool = False
		      max_tokens: int = Field(default=2048, ge=0)
		  
		      model_config = {
		          "json_schema_extra": {"examples": [{"model": "default-model", "messages": [{"role": "user", "content": "ä½ å¥½"}]}]}
		      }
		  
		  
		  async def openai_chat(msg: ChatCompletionRequest):
		      config = get_model_worker_config(msg.model)
		      openai.api_key = config.get("api_key", "EMPTY")
		      print(f"{openai.api_key=}")
		      openai.api_base = config.get("api_base_url", fschat_openai_api_address())
		      print(fschat_openai_api_address())
		      print(f"{openai.api_base=}")
		      print(msg)
		  
		      async def get_response(msg):
		          data = msg.dict()
		          ###è¿™é‡Œopenaiä¼ å‚æ–¹å¼ï¼Œè¦ä¸€ä¸ªä¸ªä¼ ï¼Œä¸èƒ½ç›´æ¥**data
		          try:
		              response = await openai.ChatCompletion.acreate(model=data['model'],
		                                          messages=data['messages'],
		                                          temperature=data['temperature'],
		                                          top_p=data['top_p'],
		                                          stream=data['stream'],
		                                          max_tokens=data['max_tokens'])
		              chat_history_id = add_chat_history_to_db(chat_type="llm_chat", query=data['messages'][0]['content'])
		              if msg.stream:
		                  async for data in response:
		                      if choices := data.choices:
		                          if chunk := choices[0].get("delta", {}).get("content"):
		                              print(chunk, end="", flush=True)
		                              ##ä¿®æ”¹è¿”å›ï¼Œè¿”å›json
		                              yield json.dumps(
		                                  {"text": chunk, "chat_history_id": chat_history_id},
		                                  ensure_ascii=False)
		              else:
		                  if response.choices:
		                      answer = response.choices[0].message.content
		                      print(answer)
		                      ##ä¿®æ”¹è¿”å›ï¼Œè¿”å›json
		                      yield json.dumps(
		                                  {"text": answer, "chat_history_id": chat_history_id},
		                                  ensure_ascii=False)
		          except Exception as e:
		              msg = f"è·å–ChatCompletionæ—¶å‡ºé”™ï¼š{e}"
		              logger.error(f'{e.__class__.__name__}: {msg}',
		                           exc_info=e if log_verbose else None)
		  
		      return StreamingResponse(
		          get_response(msg),
		          media_type='text/event-stream',
		      )
		  
		  ```
	- 3ã€åœ¨server/api.pyä¸­æŠŠå¯¹è¯éƒ¨åˆ†çš„æ¨¡å‹å…¨æ¢æˆopenai_chat
	  collapsed:: true
		-
		  ```
		  app.post("/chat/fastchat",
		               tags=["Chat"],
		               summary="ä¸llmæ¨¡å‹å¯¹è¯(ç›´æ¥ä¸fastchat apiå¯¹è¯)",
		               )(openai_chat)
		  
		      app.post("/chat/chat",
		               tags=["Chat"],
		               summary="ä¸llmæ¨¡å‹å¯¹è¯(é€šè¿‡LLMChain)",
		               )(openai_chat)
		  
		      app.post("/chat/search_engine_chat",
		               tags=["Chat"],
		               summary="ä¸æœç´¢å¼•æ“å¯¹è¯",
		               )(openai_chat)
		  ```
	- 4ã€ä¿®æ”¹ç¬¬3æ­¥ä¸­å¯¹åº”æ¥å£çš„webui_pageså¯¹åº”å‚æ•°ä¼ å‚ï¼Œä½¿å¾—ä¼ å‚ç¬¦åˆopenai_cahtã€‚å³æ˜¯ä¿®æ”¹webui_pages/utils.pyä¸­ç¬¬ä¸‰æ­¥æ‰€ä¿®æ”¹æ¥å£å¯¹åº”çš„å‡½æ•°ä¼ å‚
	  collapsed:: true
		-
		  ```
		  def chat_chat(
		          self,
		          query: str,
		          history: List[Dict] = [],
		          stream: bool = True,
		          model: str = LLM_MODELS[0],
		          temperature: float = TEMPERATURE,
		          max_tokens: int = None,
		          prompt_name: str = "default",
		          **kwargs,
		      ):
		          '''
		          å¯¹åº”api.py/chat/chatæ¥å£ #TODO: è€ƒè™‘æ˜¯å¦è¿”å›json
		          '''
		          # data = {
		          #     "query": query,
		          #     "history": history,
		          #     "stream": stream,
		          #     "model_name": model,
		          #     "temperature": temperature,
		          #     "max_tokens": max_tokens,
		          #     "prompt_name": prompt_name,
		          # }
		          data = {
		              "messages": [{'role': 'user', 'content': query}],
		              "stream": False,
		              "model": model,
		              "temperature": temperature or 0.95,
		              "max_tokens": max_tokens or 2048,
		          }
		  
		          print(f"received input message:")
		          pprint(data)
		  
		          response = self.post("/chat/chat", json=data, stream=True, **kwargs)
		          return self._httpx_stream2generator(response, as_json=True)
		  ```
	- 5ã€é¡¹ç›®å‚æ•°é…ç½®
	  collapsed:: true
		- è¦æ·»åŠ ä¸€ä¸ªç¯å¢ƒå˜é‡
			- export OPENAI_API_KEY=123456789   --linux
			- set  OPENAI_API_KEY=123456789       --windows
		- model_config.py
		  collapsed:: true
			-
			  ```
			  #æ¨¡å‹å…¨éƒ¨é…ç½®ä¸ºopenai-api
			  LLM_MODELS = ["openai-api", "openai-api", "openai-api"]
			  ```
			-
			  ```
			  ONLINE_LLM_MODEL = {
			      # çº¿ä¸Šæ¨¡å‹ã€‚è¯·åœ¨server_configä¸­ä¸ºæ¯ä¸ªåœ¨çº¿APIè®¾ç½®ä¸åŒçš„ç«¯å£
			  
			      "openai-api": {
			          "model_name": "gpt-35-turbo",
			          "api_base_url": "http://127.0.0.1:8000/v1",  ##é…ç½®chatglm cpp apiçš„æ¥å£åœ°å€
			          "api_key": "",
			          "openai_proxy": "",
			      },
			  }
			  ```
			-
			  ```
			  VLLM_MODEL_DICT = { ###é…ç½®chatglm-cpp apiéœ€è¦ç”¨åˆ°çš„æ¨¡å‹
			      "chatglm-cpp":"/root/ai/chatglm.cpp/chatglm3-6B-32K-ggml.bin", #ä¿®æ”¹ï¼Œå¯¹åº”chatglm_cpp_apiä¸­çš„æ¨¡å‹åœ°å€
			    }
			  ```
		- server_config.py
			-
			  ```
			  ##æ–°å¢chatglmcpp çš„API server,åœ¨startup.pyä¸­ç”¨åˆ°
			  CHATGLM_CPP_SERVER = {
			      "host":DEFAULT_BIND_HOST,
			      "port": 8000,
			  }
			  ```
	- 6ã€æ‰“å¼€webuiæŠ¥é”™ï¼Œè·å–ä¸åˆ°æ­£åœ¨è¿è¡Œçš„æ¨¡å‹ï¼Œæ•…ä¿®æ”¹webui_pages/dialogu/dialogue.pyï¼ŒæŠŠdefault_modeã€running_modelsã€indexç›´æ¥å†™æ­»ä¸ºopenai-api
	  collapsed:: true
		-
		  ```
		  import streamlit as st
		  from webui_pages.utils import *
		  from streamlit_chatbox import *
		  from datetime import datetime
		  import os
		  from configs import (TEMPERATURE, HISTORY_LEN, PROMPT_TEMPLATES,
		                       DEFAULT_KNOWLEDGE_BASE, DEFAULT_SEARCH_ENGINE, SUPPORT_AGENT_MODEL)
		  from typing import List, Dict
		  
		  
		  chat_box = ChatBox(
		      assistant_avatar=os.path.join(
		          "img",
		          "chatchat_icon_blue_square_v2.png"
		      )
		  )
		  
		  
		  def get_messages_history(history_len: int, content_in_expander: bool = False) -> List[Dict]:
		      '''
		      è¿”å›æ¶ˆæ¯å†å²ã€‚
		      content_in_expanderæ§åˆ¶æ˜¯å¦è¿”å›expanderå…ƒç´ ä¸­çš„å†…å®¹ï¼Œä¸€èˆ¬å¯¼å‡ºçš„æ—¶å€™å¯ä»¥é€‰ä¸Šï¼Œä¼ å…¥LLMçš„historyä¸éœ€è¦
		      '''
		  
		      def filter(msg):
		          content = [x for x in msg["elements"] if x._output_method in ["markdown", "text"]]
		          if not content_in_expander:
		              content = [x for x in content if not x._in_expander]
		          content = [x.content for x in content]
		  
		          return {
		              "role": msg["role"],
		              "content": "\n\n".join(content),
		          }
		  
		      return chat_box.filter_history(history_len=history_len, filter=filter)
		  
		  
		  def dialogue_page(api: ApiRequest, is_lite: bool = False):
		      if not chat_box.chat_inited:
		          # default_model = api.get_default_llm_model()[0]
		          #ä¿®æ”¹
		          default_model = 'openai-api'
		          st.toast(
		              f"æ¬¢è¿ä½¿ç”¨ [`Langchain-Chatchat`](https://github.com/chatchat-space/Langchain-Chatchat) ! \n\n"
		              f"å½“å‰è¿è¡Œçš„æ¨¡å‹`{default_model}`, æ‚¨å¯ä»¥å¼€å§‹æé—®äº†."
		          )
		          chat_box.init_session()
		  
		      with st.sidebar:
		          # TODO: å¯¹è¯æ¨¡å‹ä¸ä¼šè¯ç»‘å®š
		          def on_mode_change():
		              mode = st.session_state.dialogue_mode
		              text = f"å·²åˆ‡æ¢åˆ° {mode} æ¨¡å¼ã€‚"
		              if mode == "çŸ¥è¯†åº“é—®ç­”":
		                  cur_kb = st.session_state.get("selected_kb")
		                  if cur_kb:
		                      text = f"{text} å½“å‰çŸ¥è¯†åº“ï¼š `{cur_kb}`ã€‚"
		              st.toast(text)
		  
		          dialogue_modes = ["LLM å¯¹è¯",
		                              "çŸ¥è¯†åº“é—®ç­”",
		                              "æœç´¢å¼•æ“é—®ç­”",
		                              "è‡ªå®šä¹‰Agenté—®ç­”",
		                              ]
		          dialogue_mode = st.selectbox("è¯·é€‰æ‹©å¯¹è¯æ¨¡å¼ï¼š",
		                                       dialogue_modes,
		                                       index=0,
		                                       on_change=on_mode_change,
		                                       key="dialogue_mode",
		                                       )
		  
		          def on_llm_change():
		              if llm_model:
		                  config = api.get_model_config(llm_model)
		                  if not config.get("online_api"):  # åªæœ‰æœ¬åœ°model_workerå¯ä»¥åˆ‡æ¢æ¨¡å‹
		                      st.session_state["prev_llm_model"] = llm_model
		                  st.session_state["cur_llm_model"] = st.session_state.llm_model
		  
		          def llm_model_format_func(x):
		              if x in running_models:
		                  return f"{x} (Running)"
		              return x
		  
		          running_models = ['openai-api']
		          available_models = []
		          config_models = api.list_config_models()
		          worker_models = list(config_models.get("worker", {}))  # ä»…åˆ—å‡ºåœ¨FSCHAT_MODEL_WORKERSä¸­é…ç½®çš„æ¨¡å‹
		          for m in worker_models:
		              if m not in running_models and m != "default":
		                  available_models.append(m)
		          for k, v in config_models.get("online", {}).items():  # åˆ—å‡ºONLINE_MODELSä¸­ç›´æ¥è®¿é—®çš„æ¨¡å‹
		              if not v.get("provider") and k not in running_models:
		                  available_models.append(k)
		          llm_models = running_models + available_models
		          ##ä¿®æ”¹ï¼Œç›´æ¥å†™æ­»
		          print(f'hahahhahä¼¤{llm_models}')
		          index = llm_models.index('openai-api')
		          llm_model = st.selectbox("é€‰æ‹©LLMæ¨¡å‹ï¼š",
		                                   llm_models,
		                                   index,
		                                   format_func=llm_model_format_func,
		                                   on_change=on_llm_change,
		                                   key="llm_model",
		                                   )
		          if (st.session_state.get("prev_llm_model") != llm_model
		                  and not is_lite
		                  and not llm_model in config_models.get("online", {})
		                  and not llm_model in config_models.get("langchain", {})
		                  and llm_model not in running_models):
		              with st.spinner(f"æ­£åœ¨åŠ è½½æ¨¡å‹ï¼š {llm_model}ï¼Œè¯·å‹¿è¿›è¡Œæ“ä½œæˆ–åˆ·æ–°é¡µé¢"):
		                  prev_model = st.session_state.get("prev_llm_model")
		                  r = api.change_llm_model(prev_model, llm_model)
		                  if msg := check_error_msg(r):
		                      st.error(msg)
		                  elif msg := check_success_msg(r):
		                      st.success(msg)
		                      st.session_state["prev_llm_model"] = llm_model
		  
		          index_prompt = {
		              "LLM å¯¹è¯": "llm_chat",
		              "è‡ªå®šä¹‰Agenté—®ç­”": "agent_chat",
		              "æœç´¢å¼•æ“é—®ç­”": "search_engine_chat",
		              "çŸ¥è¯†åº“é—®ç­”": "knowledge_base_chat",
		          }
		          prompt_templates_kb_list = list(PROMPT_TEMPLATES[index_prompt[dialogue_mode]].keys())
		          prompt_template_name = prompt_templates_kb_list[0]
		          if "prompt_template_select" not in st.session_state:
		              st.session_state.prompt_template_select = prompt_templates_kb_list[0]
		  
		          def prompt_change():
		              text = f"å·²åˆ‡æ¢ä¸º {prompt_template_name} æ¨¡æ¿ã€‚"
		              st.toast(text)
		  
		          prompt_template_select = st.selectbox(
		              "è¯·é€‰æ‹©Promptæ¨¡æ¿ï¼š",
		              prompt_templates_kb_list,
		              index=0,
		              on_change=prompt_change,
		              key="prompt_template_select",
		          )
		          prompt_template_name = st.session_state.prompt_template_select
		          temperature = st.slider("Temperatureï¼š", 0.0, 1.0, TEMPERATURE, 0.05)
		          history_len = st.number_input("å†å²å¯¹è¯è½®æ•°ï¼š", 0, 20, HISTORY_LEN)
		  
		          def on_kb_change():
		              st.toast(f"å·²åŠ è½½çŸ¥è¯†åº“ï¼š {st.session_state.selected_kb}")
		  
		          if dialogue_mode == "çŸ¥è¯†åº“é—®ç­”":
		              with st.expander("çŸ¥è¯†åº“é…ç½®", True):
		                  kb_list = api.list_knowledge_bases()
		                  index = 0
		                  if DEFAULT_KNOWLEDGE_BASE in kb_list:
		                      index = kb_list.index(DEFAULT_KNOWLEDGE_BASE)
		                  selected_kb = st.selectbox(
		                      "è¯·é€‰æ‹©çŸ¥è¯†åº“ï¼š",
		                      kb_list,
		                      index=index,
		                      on_change=on_kb_change,
		                      key="selected_kb",
		                  )
		                  kb_top_k = st.number_input("åŒ¹é…çŸ¥è¯†æ¡æ•°ï¼š", 1, 20, VECTOR_SEARCH_TOP_K)
		  
		                  ## Bge æ¨¡å‹ä¼šè¶…è¿‡1
		                  score_threshold = st.slider("çŸ¥è¯†åŒ¹é…åˆ†æ•°é˜ˆå€¼ï¼š", 0.0, 2.0, float(SCORE_THRESHOLD), 0.01)
		  
		          elif dialogue_mode == "æœç´¢å¼•æ“é—®ç­”":
		              search_engine_list = api.list_search_engines()
		              if DEFAULT_SEARCH_ENGINE in search_engine_list:
		                  index = search_engine_list.index(DEFAULT_SEARCH_ENGINE)
		              else:
		                  index = search_engine_list.index("duckduckgo") if "duckduckgo" in search_engine_list else 0
		              with st.expander("æœç´¢å¼•æ“é…ç½®", True):
		                  search_engine = st.selectbox(
		                      label="è¯·é€‰æ‹©æœç´¢å¼•æ“",
		                      options=search_engine_list,
		                      index=index,
		                  )
		                  se_top_k = st.number_input("åŒ¹é…æœç´¢ç»“æœæ¡æ•°ï¼š", 1, 20, SEARCH_ENGINE_TOP_K)
		  
		      # Display chat messages from history on app rerun
		      chat_box.output_messages()
		  
		      chat_input_placeholder = "è¯·è¾“å…¥å¯¹è¯å†…å®¹ï¼Œæ¢è¡Œè¯·ä½¿ç”¨Shift+Enter "
		  
		      def on_feedback(
		          feedback,
		          chat_history_id: str = "",
		          history_index: int = -1,
		      ):
		          reason = feedback["text"]
		          score_int = chat_box.set_feedback(feedback=feedback, history_index=history_index)
		          api.chat_feedback(chat_history_id=chat_history_id,
		                            score=score_int,
		                            reason=reason)
		          st.session_state["need_rerun"] = True
		  
		      feedback_kwargs = {
		          "feedback_type": "thumbs",
		          "optional_text_label": "æ¬¢è¿åé¦ˆæ‚¨æ‰“åˆ†çš„ç†ç”±",
		      }
		  
		      if prompt := st.chat_input(chat_input_placeholder, key="prompt"):
		          history = get_messages_history(history_len)
		          chat_box.user_say(prompt)
		          if dialogue_mode == "LLM å¯¹è¯":
		              chat_box.ai_say("æ­£åœ¨æ€è€ƒ...")
		              text = ""
		              chat_history_id = ""
		              r = api.chat_chat(prompt,
		                                history=history,
		                                model=llm_model,
		                                prompt_name=prompt_template_name,
		                                temperature=temperature)
		              for t in r:
		                  if error_msg := check_error_msg(t):  # check whether error occured
		                      st.error(error_msg)
		                      break
		                  text += t.get("text", "")
		                  chat_box.update_msg(text)
		                  chat_history_id = t.get("chat_history_id", "")
		  
		              metadata = {
		                  "chat_history_id": chat_history_id,
		                  }
		              chat_box.update_msg(text, streaming=False, metadata=metadata)  # æ›´æ–°æœ€ç»ˆçš„å­—ç¬¦ä¸²ï¼Œå»é™¤å…‰æ ‡
		              chat_box.show_feedback(**feedback_kwargs,
		                                     key=chat_history_id,
		                                     on_submit=on_feedback,
		                                     kwargs={"chat_history_id": chat_history_id, "history_index": len(chat_box.history) - 1})
		  
		          elif dialogue_mode == "è‡ªå®šä¹‰Agenté—®ç­”":
		              if not any(agent in llm_model for agent in SUPPORT_AGENT_MODEL):
		                  chat_box.ai_say([
		                      f"æ­£åœ¨æ€è€ƒ... \n\n <span style='color:red'>è¯¥æ¨¡å‹å¹¶æ²¡æœ‰è¿›è¡ŒAgentå¯¹é½ï¼Œè¯·æ›´æ¢æ”¯æŒAgentçš„æ¨¡å‹è·å¾—æ›´å¥½çš„ä½“éªŒï¼</span>\n\n\n",
		                      Markdown("...", in_expander=True, title="æ€è€ƒè¿‡ç¨‹", state="complete"),
		  
		                  ])
		              else:
		                  chat_box.ai_say([
		                      f"æ­£åœ¨æ€è€ƒ...",
		                      Markdown("...", in_expander=True, title="æ€è€ƒè¿‡ç¨‹", state="complete"),
		  
		                  ])
		              text = ""
		              ans = ""
		              for d in api.agent_chat(prompt,
		                                      history=history,
		                                      model=llm_model,
		                                      prompt_name=prompt_template_name,
		                                      temperature=temperature,
		                                      ):
		                  try:
		                      d = json.loads(d)
		                  except:
		                      pass
		                  if error_msg := check_error_msg(d):  # check whether error occured
		                      st.error(error_msg)
		                  if chunk := d.get("answer"):
		                      text += chunk
		                      chat_box.update_msg(text, element_index=1)
		                  if chunk := d.get("final_answer"):
		                      ans += chunk
		                      chat_box.update_msg(ans, element_index=0)
		                  if chunk := d.get("tools"):
		                      text += "\n\n".join(d.get("tools", []))
		                      chat_box.update_msg(text, element_index=1)
		              chat_box.update_msg(ans, element_index=0, streaming=False)
		              chat_box.update_msg(text, element_index=1, streaming=False)
		          elif dialogue_mode == "çŸ¥è¯†åº“é—®ç­”":
		              chat_box.ai_say([
		                  f"æ­£åœ¨æŸ¥è¯¢çŸ¥è¯†åº“ `{selected_kb}` ...",
		                  Markdown("...", in_expander=True, title="çŸ¥è¯†åº“åŒ¹é…ç»“æœ", state="complete"),
		              ])
		              text = ""
		              for d in api.knowledge_base_chat(prompt,
		                                               knowledge_base_name=selected_kb,
		                                               top_k=kb_top_k,
		                                               score_threshold=score_threshold,
		                                               history=history,
		                                               model=llm_model,
		                                               prompt_name=prompt_template_name,
		                                               temperature=temperature):
		                  if error_msg := check_error_msg(d):  # check whether error occured
		                      st.error(error_msg)
		                  elif chunk := d.get("answer"):
		                      text += chunk
		                      chat_box.update_msg(text, element_index=0)
		              chat_box.update_msg(text, element_index=0, streaming=False)
		              chat_box.update_msg("\n\n".join(d.get("docs", [])), element_index=1, streaming=False)
		          elif dialogue_mode == "æœç´¢å¼•æ“é—®ç­”":
		              chat_box.ai_say([
		                  f"æ­£åœ¨æ‰§è¡Œ `{search_engine}` æœç´¢...",
		                  Markdown("...", in_expander=True, title="ç½‘ç»œæœç´¢ç»“æœ", state="complete"),
		              ])
		              text = ""
		              for d in api.search_engine_chat(prompt,
		                                              search_engine_name=search_engine,
		                                              top_k=se_top_k,
		                                              history=history,
		                                              model=llm_model,
		                                              prompt_name=prompt_template_name,
		                                              temperature=temperature,
		                                              split_result=se_top_k > 1):
		                  if error_msg := check_error_msg(d):  # check whether error occured
		                      st.error(error_msg)
		                  elif chunk := d.get("answer"):
		                      text += chunk
		                      chat_box.update_msg(text, element_index=0)
		              chat_box.update_msg(text, element_index=0, streaming=False)
		              chat_box.update_msg("\n\n".join(d.get("docs", [])), element_index=1, streaming=False)
		  
		      if st.session_state.get("need_rerun"):
		          st.session_state["need_rerun"] = False
		          st.rerun()
		  
		      now = datetime.now()
		      with st.sidebar:
		  
		          cols = st.columns(2)
		          export_btn = cols[0]
		          if cols[1].button(
		                  "æ¸…ç©ºå¯¹è¯",
		                  use_container_width=True,
		          ):
		              chat_box.reset_history()
		              st.rerun()
		  
		      export_btn.download_button(
		          "å¯¼å‡ºè®°å½•",
		          "".join(chat_box.export2md()),
		          file_name=f"{now:%Y-%m-%d %H.%M}_å¯¹è¯è®°å½•.md",
		          mime="text/markdown",
		          use_container_width=True,
		      )
		  ```

### 3. åˆå§‹åŒ–çŸ¥è¯†åº“å’Œé…ç½®æ–‡ä»¶

![](code.png)